{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Report 2: Theft Over Open Data (TOOD)\n",
    "\n",
    "> This file is intended to be used as references for sections 3 - 5 which all involves model training, \n",
    "> testing, and deployment of the model. Creativity and innovation are encouraged for this and other \n",
    "> sections moving forward.\n",
    "\n",
    "**What is the purpose of this file?**\n",
    "\n",
    "This notebook will be covering parts 3-5 of the assignment namely going over the following:\n",
    "1. Predictive Model Building - Building the Predictive Model using Modules within `sklearn`.\n",
    "2. Model Scoring and Evaluation - Evaluation and Scoring of the Model with Training Data.\n",
    "3. Model Deployment - Deployment of the model as `.pkl` files.\n",
    "\n",
    "**NOTE**: The naming of this notebook is intentionally, as it stands for the following:\n",
    "- `c309` - This is the course code COMP309.\n",
    "- `r2` - This is the report number which is report 2 of the group project.\n",
    "- `toodu` - This is the name of the dataset we named and will continue working with in this notebook.\n",
    "- `model` - This is just a generic name but this notebook will contain sections 3 - 5 which all involves model training, testing, and deployment of the model.\n",
    "\n",
    "**NOTE**: Provided below is a notebook that includes the above sections, thoroughly covering all aspects of sections 3-5. When required, there will be additional informaiton and insights throughout the notebook to help understand the specifics of the model, algorithms used, and how we handle imbalanced data when training the model."
   ],
   "id": "526803cd535af350"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "id": "dc28d89ae07e6e66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DEFAULT_DATA_PATH = os.path.join(os.pardir, \"data\")\n",
    "\n",
    "toodu_ft_df = pd.read_csv(os.path.join(DEFAULT_DATA_PATH, \"Theft_Over_Open_Data_Cleaned.csv\"))"
   ],
   "id": "dc50b8371edc6954"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Observation**: The Theft Over Dataset has a noticeable glaring issue, in that much of its data for different offense overlaps with others making it hard to predict them. As such a decision was made to merge the 6 smaller offence \n",
    "types into Theft Over. This is because many of them are already classified as sub categories of Theft Over. This Dataset sorts into 3 `UCR_CODE` categories. One is for `Theft Over`, the 2nd for `Motor Vehicle Over` and the 3rd for `Shoplifting`. `Shoplifting` however as well, shares many similarities to `Theft Over`, it's difference being that it is a theft attempted from an open retail store of merchandise. However, the two would often target the same types of locations, and the difference in charge is not a meaningful difference from `Theft Over` itself. The only one that truly differentiates itself is Theft From Motor Vehicle Over, the rest almost always being theft from similar locations.\n",
    "\n",
    "When it comes to features, while many columns appeared as though they would be useful such as using the latitude and longitude columns, or the neighbourhood codes, we found that while when included these had a higher importance than `PREMISES_CODE` or `LOCATION_CODE`, they were also a negative impact on the ability of the model to predict the outcome properly. This appears to be because it would then attempt to associate a neighbourhood with a type of crime which resulted in many improper predictions. However, adding another feature could then cause the model to basically ignore those high importance features. We found that these two features we went with, were the most relevant, and were the features that consistently didn't swing back and forth on importance and stayed very consistent between different feature combinations."
   ],
   "id": "f5b42f4c46d24a52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "theft_over_categories = {\n",
    "    \"Theft - Misapprop Funds Over\",\n",
    "    \"Theft Over - Bicycle\",\n",
    "    \"Theft Over - Distraction\",\n",
    "    \"Theft Over\",\n",
    "    \"Theft Over - Shoplifting\",\n",
    "    \"Theft Of Utilities Over\",\n",
    "    \"Theft From Mail / Bag / Key\"\n",
    "}\n",
    "\n",
    "toodu_ft_df[\"OFFENCE\"] = toodu_ft_df[\"OFFENCE\"].replace(theft_over_categories, \"Theft Over\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "toodu_ft_df[\"OFFENCE_ENCODED\"] = le.fit_transform(toodu_ft_df[\"OFFENCE\"])\n",
    "\n",
    "categorical_features = [\n",
    "    \"PREMISES_TYPE\",\n",
    "    \"LOCATION_TYPE\"\n",
    "]\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    toodu_ft_df[col] = label_encoders[col].fit_transform(toodu_ft_df[col])"
   ],
   "id": "fa344f6f8164feeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "features_filtered = toodu_ft_df[categorical_features]\n",
    "features_filtered"
   ],
   "id": "a5a3ebd2208172dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "target_filtered = toodu_ft_df[\"OFFENCE_ENCODED\"]\n",
    "target_filtered"
   ],
   "id": "9d30874836f94b95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "numerical_features_sf = scaler.fit_transform(features_filtered)\n",
    "numerical_features_sf"
   ],
   "id": "8c94835081c6d7dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(numerical_features_sf, target_filtered, test_size=0.2, random_state=42)"
   ],
   "id": "eec7214049ba7d26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install imbalanced-learn",
   "id": "39badd1337c7767"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# NOTE: We can use a better `sampling_strategy` than just \"auto\"\n",
    "smote = SMOTE(random_state=42, sampling_strategy=\"auto\")\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)"
   ],
   "id": "ce29adb6869c9bbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "SAMPLES_PER_CLASS = 1500\n",
    "\n",
    "unique_classes = np.unique(y_train).tolist()\n",
    "sampling_strategy = {cls: min(SAMPLES_PER_CLASS, (y_train == cls).sum()) for cls in unique_classes}\n",
    "\n",
    "under_sampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "x_train, y_train = under_sampler.fit_resample(x_train, y_train)"
   ],
   "id": "3ddbfe8ed953cd16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# NOTE: Start of Model Creating and Training\n",
    "\n",
    "# NOTE: Also can add the following: n_jobs=5, C=100 | C=0.01\n",
    "model = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "score_model = lambda model, x_test, y_test: model.score(x_test, y_test)\n",
    "score_model(model, x_test, y_test)"
   ],
   "id": "25396efe6f2f6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# NOTE: Start of Model Scoring and Evaluation (Classification Reports)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_proba = model.predict_proba(x_test)[:10]\n",
    "y_pred_inverse = le.inverse_transform(y_pred)\n",
    "y_test_inverse = le.inverse_transform(y_test)\n",
    "\n",
    "print(f\"Classification Report: {classification_report(y_test, y_pred)}\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Training Data Score: {model.score(x_train, y_train)}\")\n",
    "print(f\"Testing Data Score (Overall Accuracy): {model.score(x_test, y_test)}\")"
   ],
   "id": "1dc8be1b3d9dc2a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_test.value_counts()",
   "id": "9e8aeb1b7f4c6a8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_pred_proba",
   "id": "5193ef531b888976"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "null_accuracy_score = (2092/(2092 + 700))\n",
    "print(f\"Null Accuracy Score: {null_accuracy_score}\")"
   ],
   "id": "bae8ca39fb12e04e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# NOTE: Start of Confusion Matrix and Visualizations\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TODO: FIX THE LABELS!\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "labels = [\"True Negative\", \"False Positive\", \"False Negative\", \"True Positive\"]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, cmap=\"YlOrRd\", fmt=\"d\")\n",
    "plt.title(\"Confusion Matrix of Theft Incidents\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.xticks([0.5, 1.5], labels[:2],rotation=0)\n",
    "plt.yticks([0.5, 1.5], labels[2:],rotation=90)\n",
    "plt.show()"
   ],
   "id": "dcd3cefa6b7a1263"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# NOTE: Classification Accuracy and Error\n",
    "TP = cm[0, 0]\n",
    "TN = cm[1, 1]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "precision_score = TP / float(TP + FP)\n",
    "recall_score = TP / float(TP + FN)\n",
    "true_positive_rate = TP / float(TP + FN)\n",
    "false_positive_rate = FP / float(FP + TN)\n",
    "\n",
    "print(f\"\"\"\n",
    "Classification Accuracy: {classification_accuracy}\n",
    "Classification Error:    {classification_error}\n",
    "Precision Score:         {precision_score}\n",
    "Recall Score:            {recall_score}\n",
    "True Positive Rate:      {true_positive_rate}\n",
    "False Positive Rate:     {false_positive_rate}\n",
    "\"\"\")"
   ],
   "id": "26b5bb04487c9ec0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TODO: ROC (Receiver Operation Characteristics) Curve Here.",
   "id": "75e3c27231d799c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TODO: Feature Importances Here.",
   "id": "d4ce5c74937f533"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:59:19.892290Z",
     "start_time": "2024-11-28T16:59:19.677963Z"
    }
   },
   "cell_type": "code",
   "source": "toodu_ft_df.to_csv(os.path.join(DEFAULT_DATA_PATH, \"Theft_Over_Data_Cleaned_Encoded.csv\"), index=False)",
   "id": "ea91c26541f6f732",
   "outputs": [],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
